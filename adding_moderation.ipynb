{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This moderation is a copied from this [github repo from Pere Martra](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/3-LangChain/LangChain_OpenAI_Moderation_System.ipynb). The Langchain cookbook moderation fails due to the update OpenAI policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PrompTemplate is a custom class that provides funcrionality to create prompts\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assistant_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction how the LLM must respond the comments,\n",
    "assistant_template = \"\"\"\n",
    "You are {sentiment} assistant that responds to user comments,\n",
    "using similar vocabulary than the user.\n",
    "User:\" {customer_request}\"\n",
    "Comment:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the prompt template to use in the Chain for the first Model.\n",
    "assistant_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"sentiment\", \"customer_request\"],\n",
    "    template=assistant_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_chain = LLMChain(\n",
    "    llm=assistant_llm,\n",
    "    prompt=assistant_prompt_template,\n",
    "    output_key=\"assistant_response\",\n",
    "    verbose=False,\n",
    ")\n",
    "#the output of the formatted prompt will pass directly to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support function to obtain a response to a user comment.\n",
    "def create_dialog(customer_request, sentiment):\n",
    "    #calling the .invoke method from the chain created Above.\n",
    "    assistant_response = assistant_chain.invoke(\n",
    "        {\"customer_request\": customer_request,\n",
    "        \"sentiment\": sentiment}\n",
    "    )\n",
    "    return assistant_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the customer request, or customer comment in the forum moderated by the agent.\n",
    "# feel free to modify it.\n",
    "customer_request = \"\"\"This product is a piece of shit. I feel like an Idiot!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant response: I totally understand your frustration with this product. It can be really disappointing when something doesn't live up to our expectations.\n"
     ]
    }
   ],
   "source": [
    "# Our assistant working in 'nice' mode.\n",
    "response_data=create_dialog(customer_request, \"nice\")\n",
    "print(f\"assistant response: {response_data['assistant_response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant response: Well, it seems like you have quite the negative opinion about this product. It appears to be causing you quite a bit of frustration and disappointment.\n"
     ]
    }
   ],
   "source": [
    "#Our assistant running in rude mode.\n",
    "response_data = create_dialog(customer_request, \"very rude\")\n",
    "print(f\"assistant response: {response_data['assistant_response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moderator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The moderator prompt template\n",
    "moderator_template = \"\"\"\n",
    "You are the moderator of an online forum, you are strict and will not tolerate any negative comments.\n",
    "You will receive a Original comment and if it is impolite you must transform in polite.\n",
    "Try to mantain the meaning when possible,\n",
    "\n",
    "If it it's polite, you will let it remain as is and repeat it word for word.\n",
    "Original comment: {comment_to_moderate}\n",
    "\"\"\"\n",
    "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
    "moderator_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"comment_to_moderate\"],\n",
    "    template=moderator_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm going to use a more advanced LLM\n",
    "moderator_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We build the chain for the moderator.\n",
    "moderator_chain = LLMChain(\n",
    "    llm=moderator_llm, prompt=moderator_prompt_template, verbose=False\n",
    ")  # the output of the prompt will pass to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run our chain we use the .run() command\n",
    "moderator_data = moderator_chain.invoke({\"comment_to_moderate\": response_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: {'customer_request': 'This product is a piece of shit. I feel like an Idiot!', 'sentiment': 'very rude', 'assistant_response': 'Well, it seems like you have quite a negative opinion about this product. It appears to be causing you quite a bit of frustration and disappointment.'}\n"
     ]
    }
   ],
   "source": [
    "print(moderator_data['text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The optput of the first chain must coincide with one of the parameters of the second chain.\n",
    "#The parameter is defined in the prompt_template.\n",
    "assistant_chain = LLMChain(\n",
    "    llm=assistant_llm,\n",
    "    prompt=assistant_prompt_template,\n",
    "    output_key=\"comment_to_moderate\",\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "#verbose True because we want to see the intermediate messages.\n",
    "moderator_chain = LLMChain(\n",
    "    llm=moderator_llm,\n",
    "    prompt=moderator_prompt_template,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# Creating the SequentialChain class indicating chains and parameters.\n",
    "assistant_moderated_chain = SequentialChain(\n",
    "    chains=[assistant_chain, moderator_chain],\n",
    "    input_variables=[\"sentiment\", \"customer_request\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are the moderator of an online forum, you are strict and will not tolerate any negative comments.\n",
      "You will receive a Original comment and if it is impolite you must transform in polite.\n",
      "Try to mantain the meaning when possible,\n",
      "\n",
      "If it it's polite, you will let it remain as is and repeat it word for word.\n",
      "Original comment: I understand that you're feeling frustrated with this product and it's making you question your decision. I'm here to help, so please let me know how I can assist you in resolving any issues you may be facing.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'polite',\n",
       " 'customer_request': 'This product is a piece of shit. I feel like an Idiot!',\n",
       " 'text': \"I understand that you're feeling frustrated with this product and it's making you question your decision. I'm here to help, so please let me know how I can assist you in resolving any issues you may be facing.\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now run the chain.\n",
    "assistant_moderated_chain.invoke({\"sentiment\": \"polite\", \"customer_request\": customer_request})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are the moderator of an online forum, you are strict and will not tolerate any negative comments.\n",
      "You will receive a Original comment and if it is impolite you must transform in polite.\n",
      "Try to mantain the meaning when possible,\n",
      "\n",
      "If it it's polite, you will let it remain as is and repeat it word for word.\n",
      "Original comment: I understand that you're disappointed with this product and feel frustrated. I'm sorry to hear that you're not satisfied with your purchase. Is there anything specific that I can assist you with or any feedback you'd like to share?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sentiment': 'polite',\n",
       " 'customer_request': 'This product is a piece of shit. I feel like an Idiot!',\n",
       " 'text': 'I appreciate your understanding and recognition of the disappointment and frustration that the product has caused you. I apologize for any inconvenience this may have caused and I would like to offer my assistance in any way possible. Please let me know if there is anything specific I can do to help or if you have any feedback you would like to share.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now run the chain.\n",
    "assistant_moderated_chain.invoke({\"sentiment\": \"polite\", \"customer_request\": customer_request})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
